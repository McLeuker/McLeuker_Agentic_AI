"""
Manus-Style Execution Orchestrator
===================================

Coordinates Planner, Executor, and Verifier agents to autonomously complete tasks.

Architecture:
- Planner Agent (Kimi 2.5 / Grok): Decomposes tasks into executable steps
- Executor Agent (E2B + Browserless + SearchLayer): Executes code and browser actions
- Verifier Agent (Grok / Kimi): Checks results and triggers corrections
- Delivery Agent (Kimi 2.5): Packages and delivers final output

Adapted for McLeuker AI – uses existing sync OpenAI clients via run_in_executor.
"""

import asyncio
import json
import uuid
import traceback
from typing import List, Dict, Any, Optional, Callable, Literal, AsyncGenerator
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class ExecutionStatus(Enum):
    """Execution status states"""
    PENDING = "pending"
    PLANNING = "planning"
    RESEARCHING = "researching"
    EXECUTING = "executing"
    VERIFYING = "verifying"
    RETRYING = "retrying"
    DELIVERING = "delivering"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    PAUSED = "paused"


class StepType(Enum):
    """Types of execution steps"""
    PLAN = "plan"
    RESEARCH = "research"
    CODE = "code"
    BROWSER = "browser"
    VERIFY = "verify"
    DELIVER = "deliver"
    THINK = "think"


@dataclass
class ExecutionArtifact:
    """Generated artifact from execution"""
    artifact_id: str
    name: str
    type: Literal["code", "document", "image", "data", "other"]
    content: Optional[str] = None
    file_path: Optional[str] = None
    public_url: Optional[str] = None
    size_bytes: int = 0
    created_at: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ExecutionStep:
    """Individual execution step"""
    step_id: str
    execution_id: str
    step_number: int
    step_type: StepType
    status: ExecutionStatus
    agent: Literal["kimi", "grok"]
    instruction: str
    input_data: Dict[str, Any] = field(default_factory=dict)
    output_data: Any = None
    reasoning: str = ""
    artifacts: List[ExecutionArtifact] = field(default_factory=list)
    error_message: Optional[str] = None
    retry_count: int = 0
    max_retries: int = 3
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    dependencies: List[str] = field(default_factory=list)


@dataclass
class ExecutionPlan:
    """Execution plan generated by Planner Agent"""
    plan_id: str
    objective: str
    steps: List[Dict[str, Any]]
    estimated_duration: int  # seconds
    reasoning: str
    parallel_groups: List[List[Any]]  # Step numbers/IDs that can run in parallel


@dataclass
class ExecutionResult:
    """Final execution result"""
    execution_id: str
    status: ExecutionStatus
    user_request: str
    final_output: str
    artifacts: List[ExecutionArtifact]
    steps: List[ExecutionStep]
    started_at: datetime
    completed_at: Optional[datetime]
    total_tokens: int
    execution_time_seconds: float
    metadata: Dict[str, Any]


class ExecutionOrchestrator:
    """
    Manus AI-style execution orchestrator for McLeuker.

    Provides end-to-end task execution with:
    - Automatic planning with LLM
    - Parallel execution of independent steps
    - Self-correction with retry logic
    - Real-time progress streaming via SSE/WebSocket
    - Integration with E2B, Browserless, and SearchLayer
    """

    def __init__(
        self,
        kimi_client=None,
        grok_client=None,
        search_layer=None,
        e2b_manager=None,
        browserless_client=None,
        max_steps: int = 15,
        enable_auto_correct: bool = True,
    ):
        """
        Initialize execution orchestrator.

        All clients are optional – the orchestrator gracefully degrades
        when a client is unavailable.
        """
        self.kimi = kimi_client
        self.grok = grok_client
        self.search_layer = search_layer
        self.e2b = e2b_manager
        self.browserless = browserless_client
        self.max_steps = max_steps
        self.enable_auto_correct = enable_auto_correct

        # Active executions
        self._executions: Dict[str, Dict[str, Any]] = {}
        self._callbacks: Dict[str, List[Callable]] = {}

        logger.info("Execution orchestrator initialized")

    # ------------------------------------------------------------------
    # Primary LLM helper
    # ------------------------------------------------------------------

    def _get_primary_client(self):
        """Get the best available LLM client (sync OpenAI)."""
        return self.kimi or self.grok

    async def _llm_call(self, messages: List[Dict], temperature: float = 0.7, max_tokens: int = 16384) -> str:
        """Make an LLM call using the best available client."""
        client = self._get_primary_client()
        if not client:
            return "No LLM client available"

        import functools
        try:
            model = "kimi-k2.5" if client == self.kimi else "grok-3-mini"
            temp = 1 if client == self.kimi else temperature

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                functools.partial(
                    client.chat.completions.create,
                    model=model,
                    messages=messages,
                    temperature=temp,
                    max_tokens=max_tokens,
                ),
            )
            return response.choices[0].message.content or ""
        except Exception as e:
            # Try fallback
            fallback = self.grok if client == self.kimi else self.kimi
            if fallback:
                try:
                    model = "grok-3-mini" if fallback == self.grok else "kimi-k2.5"
                    temp = 0.5 if fallback == self.grok else 1
                    loop = asyncio.get_event_loop()
                    response = await loop.run_in_executor(
                        None,
                        functools.partial(
                            fallback.chat.completions.create,
                            model=model,
                            messages=messages,
                            temperature=temp,
                            max_tokens=max_tokens,
                        ),
                    )
                    return response.choices[0].message.content or ""
                except Exception as e2:
                    return f"LLM error: {str(e)} / fallback: {str(e2)}"
            return f"LLM error: {str(e)}"

    # ------------------------------------------------------------------
    # Main execution
    # ------------------------------------------------------------------

    async def execute(
        self,
        user_request: str,
        context: Optional[Dict[str, Any]] = None,
        execution_id: Optional[str] = None,
        stream_callback: Optional[Callable] = None,
    ) -> ExecutionResult:
        """Execute a user request end-to-end."""

        execution_id = execution_id or f"exec_{uuid.uuid4().hex[:12]}"
        start_time = datetime.now()

        self._executions[execution_id] = {
            "status": ExecutionStatus.PLANNING,
            "current_step": 0,
            "total_steps": 0,
            "user_request": user_request,
            "context": context or {},
            "artifacts": [],
            "steps": [],
            "started_at": start_time,
            "cancelled": False,
            "paused": False,
        }

        if stream_callback:
            self._callbacks[execution_id] = [stream_callback]

        try:
            # Phase 1: Planning
            await self._emit(execution_id, "execution.started", {
                "execution_id": execution_id,
                "request": user_request,
            })

            plan = await self._create_plan(execution_id, user_request, context)

            await self._emit(execution_id, "execution.plan_created", {
                "plan_id": plan.plan_id,
                "estimated_steps": len(plan.steps),
                "estimated_duration": plan.estimated_duration,
                "reasoning": plan.reasoning[:500],
            })

            # Phase 2: Execution
            self._executions[execution_id]["status"] = ExecutionStatus.EXECUTING
            self._executions[execution_id]["total_steps"] = len(plan.steps)

            executed_steps = await self._execute_plan(execution_id, plan, context)

            # Phase 3: Verification
            self._executions[execution_id]["status"] = ExecutionStatus.VERIFYING
            verification_result = await self._verify_execution(execution_id, executed_steps, user_request)

            # Phase 4: Delivery
            self._executions[execution_id]["status"] = ExecutionStatus.DELIVERING
            final_output = await self._deliver_result(execution_id, executed_steps, user_request, verification_result)

            # Complete
            completed_at = datetime.now()
            execution_time = (completed_at - start_time).total_seconds()

            self._executions[execution_id]["status"] = ExecutionStatus.COMPLETED
            self._executions[execution_id]["completed_at"] = completed_at

            await self._emit(execution_id, "execution.completed", {
                "execution_id": execution_id,
                "execution_time_seconds": execution_time,
                "total_steps": len(executed_steps),
                "artifacts_count": len(self._executions[execution_id]["artifacts"]),
            })

            return ExecutionResult(
                execution_id=execution_id,
                status=ExecutionStatus.COMPLETED,
                user_request=user_request,
                final_output=final_output,
                artifacts=self._executions[execution_id]["artifacts"],
                steps=executed_steps,
                started_at=start_time,
                completed_at=completed_at,
                total_tokens=0,
                execution_time_seconds=execution_time,
                metadata={"verification": verification_result},
            )

        except Exception as e:
            logger.error(f"Execution failed: {e}")
            logger.error(traceback.format_exc())

            self._executions[execution_id]["status"] = ExecutionStatus.FAILED

            await self._emit(execution_id, "execution.failed", {
                "execution_id": execution_id,
                "error": str(e),
            })

            return ExecutionResult(
                execution_id=execution_id,
                status=ExecutionStatus.FAILED,
                user_request=user_request,
                final_output=f"Execution failed: {str(e)}",
                artifacts=[],
                steps=self._executions[execution_id].get("steps", []),
                started_at=start_time,
                completed_at=datetime.now(),
                total_tokens=0,
                execution_time_seconds=(datetime.now() - start_time).total_seconds(),
                metadata={"error": str(e)},
            )

    # ------------------------------------------------------------------
    # Streaming execution (for SSE endpoints)
    # ------------------------------------------------------------------

    async def execute_stream(
        self,
        user_request: str,
        context: Optional[Dict[str, Any]] = None,
        execution_id: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """Execute with SSE streaming – yields data: JSON events."""

        execution_id = execution_id or f"exec_{uuid.uuid4().hex[:12]}"
        events: asyncio.Queue = asyncio.Queue()

        async def _callback(event_type: str, data: Dict):
            await events.put({"type": event_type, "data": data})

        # Start execution in background
        task = asyncio.create_task(
            self.execute(user_request, context, execution_id, stream_callback=_callback)
        )

        # Stream events
        while not task.done() or not events.empty():
            try:
                evt = await asyncio.wait_for(events.get(), timeout=1.0)
                yield f"data: {json.dumps(evt)}\n\n"
            except asyncio.TimeoutError:
                # Send heartbeat
                yield f"data: {json.dumps({'type': 'heartbeat', 'data': {'execution_id': execution_id}})}\n\n"

        # Get final result
        result = task.result()
        yield f"data: {json.dumps({'type': 'execution.result', 'data': {'execution_id': result.execution_id, 'status': result.status.value, 'output': result.final_output[:2000], 'execution_time': result.execution_time_seconds}})}\n\n"
        yield "data: [DONE]\n\n"

    # ------------------------------------------------------------------
    # Planning
    # ------------------------------------------------------------------

    async def _create_plan(self, execution_id: str, user_request: str, context: Optional[Dict]) -> ExecutionPlan:
        """Create execution plan using LLM."""

        await self._emit(execution_id, "planning.started", {"message": "Creating execution plan..."})

        from datetime import date
        current_date = date.today().isoformat()

        plan_prompt = f"""You are an execution planner for an AI agent system. Today is {current_date}.

Decompose this user request into executable steps. Each step should be one of:
- research: Search the web for information
- code: Generate or execute code
- browser: Navigate and extract web content
- think: Analyze and reason about data
- plan: Sub-planning for complex parts

For each step provide:
- step_number (integer)
- step_type (research/code/browser/think/plan)
- instruction (specific, actionable)
- agent (kimi or grok)
- expected_output (what this step produces)
- dependencies (list of step_numbers this depends on, empty if independent)

Also identify parallel_groups: groups of step_numbers that can execute simultaneously.

Respond in JSON:
{{
    "objective": "brief description",
    "reasoning": "why this decomposition",
    "steps": [...],
    "parallel_groups": [[1, 2], [3]]
}}

User Request: {user_request}
Context: {json.dumps(context) if context else 'None'}"""

        content = await self._llm_call(
            messages=[{"role": "user", "content": plan_prompt}],
            max_tokens=8192,
        )

        try:
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]

            plan_data = json.loads(content.strip())

            steps = plan_data.get("steps", [])
            # Ensure step_type is valid
            valid_types = {st.value for st in StepType}
            for s in steps:
                if s.get("step_type") not in valid_types:
                    s["step_type"] = "think"
                if "agent" not in s:
                    s["agent"] = "kimi"

            plan = ExecutionPlan(
                plan_id=f"plan_{uuid.uuid4().hex[:8]}",
                objective=plan_data.get("objective", user_request),
                steps=steps[:self.max_steps],
                estimated_duration=len(steps) * 30,
                reasoning=plan_data.get("reasoning", ""),
                parallel_groups=plan_data.get("parallel_groups", [[s["step_number"]] for s in steps]),
            )

        except (json.JSONDecodeError, KeyError) as e:
            logger.warning(f"Plan parsing failed, using fallback: {e}")
            plan = ExecutionPlan(
                plan_id=f"plan_{uuid.uuid4().hex[:8]}",
                objective=user_request,
                steps=[{
                    "step_number": 1,
                    "step_type": "research",
                    "instruction": user_request,
                    "agent": "kimi",
                    "expected_output": "Complete response",
                    "dependencies": [],
                }],
                estimated_duration=30,
                reasoning="Fallback single-step plan",
                parallel_groups=[[1]],
            )

        await self._emit(execution_id, "planning.completed", {
            "plan_id": plan.plan_id,
            "steps": len(plan.steps),
            "reasoning": plan.reasoning[:300],
        })

        return plan

    # ------------------------------------------------------------------
    # Execution
    # ------------------------------------------------------------------

    async def _execute_plan(self, execution_id: str, plan: ExecutionPlan, context: Optional[Dict]) -> List[ExecutionStep]:
        """Execute plan steps, respecting dependencies and parallelism."""

        executed_steps: List[ExecutionStep] = []
        completed_ids: set = set()

        for group in plan.parallel_groups:
            if self._executions[execution_id].get("cancelled"):
                break

            group_steps = [s for s in plan.steps if s.get("step_number") in group or str(s.get("step_number")) in [str(g) for g in group]]

            if not group_steps:
                continue

            # Check dependencies
            can_run = True
            for s in group_steps:
                for dep in s.get("dependencies", []):
                    if str(dep) not in completed_ids:
                        can_run = False
                        break
            if not can_run:
                continue

            # Execute in parallel
            if len(group_steps) > 1:
                results = await asyncio.gather(
                    *[self._execute_step(execution_id, s, context, executed_steps) for s in group_steps],
                    return_exceptions=True,
                )
                for r in results:
                    if isinstance(r, ExecutionStep):
                        executed_steps.append(r)
                    elif isinstance(r, Exception):
                        logger.error(f"Step execution exception: {r}")
            else:
                result = await self._execute_step(execution_id, group_steps[0], context, executed_steps)
                executed_steps.append(result)

            for s in group_steps:
                completed_ids.add(str(s.get("step_number")))

        return executed_steps

    async def _execute_step(
        self,
        execution_id: str,
        step_data: Dict[str, Any],
        context: Optional[Dict],
        previous_steps: List[ExecutionStep],
    ) -> ExecutionStep:
        """Execute a single step with auto-correction."""

        step_id = f"step_{uuid.uuid4().hex[:8]}"
        step_number = step_data.get("step_number", 0)
        step_type_str = step_data.get("step_type", "think")
        step_type = StepType(step_type_str) if step_type_str in [st.value for st in StepType] else StepType.THINK

        step = ExecutionStep(
            step_id=step_id,
            execution_id=execution_id,
            step_number=step_number,
            step_type=step_type,
            status=ExecutionStatus.EXECUTING,
            agent=step_data.get("agent", "kimi"),
            instruction=step_data.get("instruction", ""),
            input_data=step_data,
        )

        self._executions[execution_id]["current_step"] = step_number

        await self._emit(execution_id, "execution.step_started", {
            "step_id": step_id,
            "step_number": step_number,
            "step_type": step_type.value,
            "instruction": step.instruction[:200],
        })

        step.started_at = datetime.now()

        while step.retry_count < step.max_retries:
            try:
                if step_type == StepType.RESEARCH:
                    result = await self._execute_research_step(step, context, previous_steps)
                elif step_type == StepType.CODE:
                    result = await self._execute_code_step(step, context)
                elif step_type == StepType.BROWSER:
                    result = await self._execute_browser_step(step, context)
                elif step_type == StepType.THINK:
                    result = await self._execute_think_step(step, context, previous_steps)
                else:
                    result = await self._execute_generic_step(step, context, previous_steps)

                step.output_data = result
                step.status = ExecutionStatus.COMPLETED
                step.completed_at = datetime.now()

                await self._emit(execution_id, "execution.step_completed", {
                    "step_id": step_id,
                    "step_number": step_number,
                    "result_summary": str(result)[:300] if result else None,
                })
                return step

            except Exception as e:
                step.error_message = str(e)
                step.retry_count += 1
                logger.warning(f"Step {step_number} failed (attempt {step.retry_count}): {e}")

                if step.retry_count >= step.max_retries:
                    step.status = ExecutionStatus.FAILED
                    step.completed_at = datetime.now()
                    await self._emit(execution_id, "execution.step_failed", {
                        "step_id": step_id,
                        "step_number": step_number,
                        "error": str(e),
                    })
                    return step

                if self.enable_auto_correct:
                    step.status = ExecutionStatus.RETRYING
                    await self._emit(execution_id, "execution.retrying", {
                        "step_id": step_id,
                        "attempt": step.retry_count,
                        "error": str(e),
                    })
                    await asyncio.sleep(1 * step.retry_count)

        return step

    # ------------------------------------------------------------------
    # Step executors
    # ------------------------------------------------------------------

    async def _execute_research_step(self, step: ExecutionStep, context: Optional[Dict], previous_steps: List[ExecutionStep]) -> Dict:
        """Execute research using SearchLayer."""
        if self.search_layer:
            results = await self.search_layer.search(
                step.instruction,
                sources=["web", "news", "social"],
                num_results=15,
            )
            return {"search_results": results}
        else:
            # Fallback to LLM
            content = await self._llm_call([
                {"role": "system", "content": "You are a research assistant. Provide comprehensive, factual information."},
                {"role": "user", "content": step.instruction},
            ])
            return {"research": content}

    async def _execute_code_step(self, step: ExecutionStep, context: Optional[Dict]) -> Dict:
        """Execute code in E2B sandbox or generate via LLM."""
        # Generate code
        code_content = await self._llm_call([
            {"role": "system", "content": "You are a code generation assistant. Write clean, executable Python code. Return ONLY the code, no explanations."},
            {"role": "user", "content": step.instruction},
        ])

        # Try E2B execution
        if self.e2b and self.e2b.available:
            result = await self.e2b.execute_code(code_content)
            return {
                "code": code_content,
                "execution": {"success": result.success, "output": result.output, "error": result.error},
            }

        return {"code": code_content, "execution": {"note": "E2B not available, code generated but not executed"}}

    async def _execute_browser_step(self, step: ExecutionStep, context: Optional[Dict]) -> Dict:
        """Execute browser automation via Browserless."""
        if self.browserless and self.browserless.available:
            # Extract URL from instruction
            import re
            urls = re.findall(r'https?://[^\s<>"{}|\\^`\[\]]+', step.instruction)
            if urls:
                result = await self.browserless.deep_extract(urls[0])
                return result

            # No URL found, use navigate with search
            result = await self.browserless.navigate(f"https://www.google.com/search?q={step.instruction[:100]}")
            return {"content": result.content[:5000] if result.content else "", "success": result.success}

        # Fallback to LLM
        content = await self._llm_call([
            {"role": "system", "content": "You are a web browsing assistant. Extract and summarize web content."},
            {"role": "user", "content": step.instruction},
        ])
        return {"data": content}

    async def _execute_think_step(self, step: ExecutionStep, context: Optional[Dict], previous_steps: List[ExecutionStep]) -> Dict:
        """Execute thinking/analysis step."""
        # Include previous step outputs for context
        prev_context = ""
        for ps in previous_steps[-3:]:
            if ps.output_data:
                prev_context += f"\n[Step {ps.step_number} ({ps.step_type.value})]: {str(ps.output_data)[:500]}"

        content = await self._llm_call([
            {"role": "system", "content": "You are an analytical assistant. Provide deep, structured analysis."},
            {"role": "user", "content": f"{step.instruction}\n\nPrevious context:{prev_context}"},
        ])
        return {"analysis": content}

    async def _execute_generic_step(self, step: ExecutionStep, context: Optional[Dict], previous_steps: List[ExecutionStep]) -> Dict:
        """Execute generic step."""
        content = await self._llm_call([{"role": "user", "content": step.instruction}])
        return {"output": content}

    # ------------------------------------------------------------------
    # Verification
    # ------------------------------------------------------------------

    async def _verify_execution(self, execution_id: str, steps: List[ExecutionStep], user_request: str) -> Dict:
        """Verify execution results."""
        await self._emit(execution_id, "verification.started", {"message": "Verifying results..."})

        step_outputs = []
        for s in steps:
            if s.output_data:
                step_outputs.append({
                    "step": s.step_number,
                    "type": s.step_type.value,
                    "status": s.status.value,
                    "output": str(s.output_data)[:300],
                })

        verification_prompt = f"""Verify execution results for: "{user_request}"

Steps completed:
{json.dumps(step_outputs, indent=2)}

Check: 1) Completeness 2) Accuracy 3) Coherence

Respond in JSON:
{{"verified": true, "confidence": 0.9, "issues": [], "recommendations": []}}"""

        content = await self._llm_call([{"role": "user", "content": verification_prompt}])

        try:
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            result = json.loads(content.strip())
        except (json.JSONDecodeError, IndexError):
            result = {"verified": True, "confidence": 0.5, "issues": [], "recommendations": []}

        await self._emit(execution_id, "verification.completed", {
            "verified": result.get("verified", True),
            "confidence": result.get("confidence", 0.5),
        })

        return result

    # ------------------------------------------------------------------
    # Delivery
    # ------------------------------------------------------------------

    async def _deliver_result(self, execution_id: str, steps: List[ExecutionStep], user_request: str, verification: Dict) -> str:
        """Generate final deliverable."""
        await self._emit(execution_id, "delivery.started", {"message": "Preparing final output..."})

        step_summaries = []
        for s in steps:
            step_summaries.append({
                "step": s.step_number,
                "type": s.step_type.value,
                "status": s.status.value,
                "output": str(s.output_data)[:500] if s.output_data else "No output",
            })

        delivery_prompt = f"""Synthesize a final, comprehensive response for the user.

User Request: {user_request}

Execution Results:
{json.dumps(step_summaries, indent=2)}

Verification: {json.dumps(verification)}

Provide:
1. Direct answer to the user's request
2. Key findings and results
3. Summary of what was accomplished

Format clearly and professionally. Use markdown formatting."""

        content = await self._llm_call(
            messages=[{"role": "user", "content": delivery_prompt}],
            max_tokens=16384,
        )

        await self._emit(execution_id, "delivery.completed", {"output_length": len(content)})
        return content

    # ------------------------------------------------------------------
    # Event emission
    # ------------------------------------------------------------------

    async def _emit(self, execution_id: str, event: str, data: Dict):
        """Emit event to all registered callbacks."""
        if execution_id in self._callbacks:
            for cb in self._callbacks[execution_id]:
                try:
                    if asyncio.iscoroutinefunction(cb):
                        await cb(event, data)
                    else:
                        cb(event, data)
                except Exception as e:
                    logger.error(f"Callback error: {e}")

    # ------------------------------------------------------------------
    # Control methods
    # ------------------------------------------------------------------

    async def pause_execution(self, execution_id: str):
        if execution_id in self._executions:
            self._executions[execution_id]["paused"] = True
            self._executions[execution_id]["status"] = ExecutionStatus.PAUSED
            await self._emit(execution_id, "execution.paused", {"execution_id": execution_id})

    async def resume_execution(self, execution_id: str):
        if execution_id in self._executions:
            self._executions[execution_id]["paused"] = False
            self._executions[execution_id]["status"] = ExecutionStatus.EXECUTING
            await self._emit(execution_id, "execution.resumed", {"execution_id": execution_id})

    async def cancel_execution(self, execution_id: str):
        if execution_id in self._executions:
            self._executions[execution_id]["cancelled"] = True
            self._executions[execution_id]["status"] = ExecutionStatus.CANCELLED
            await self._emit(execution_id, "execution.cancelled", {"execution_id": execution_id})

    def get_execution_status(self, execution_id: str) -> Optional[Dict]:
        exec_data = self._executions.get(execution_id)
        if not exec_data:
            return None
        return {
            "execution_id": execution_id,
            "status": exec_data["status"].value if isinstance(exec_data["status"], ExecutionStatus) else exec_data["status"],
            "current_step": exec_data.get("current_step", 0),
            "total_steps": exec_data.get("total_steps", 0),
            "started_at": exec_data["started_at"].isoformat() if exec_data.get("started_at") else None,
        }

    def list_executions(self) -> List[Dict]:
        return [self.get_execution_status(eid) for eid in self._executions if self.get_execution_status(eid)]
